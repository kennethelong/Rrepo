---
title: "Exam01"
author: "Kenneth"
format: html
editor: visual
embed-ressources: true
---

**Data science exam: part 2**

# **Part II - 1: Regression**

**Introduction - Read Me**

This file contains the work of **Kenneth Elong**. It was submitted for review on the 15th of October.

All exercises were solved by Kenneth Elong. No one else have provided any code for the solving of the exercises. I did however discuss possible solutions and errors I encountered with a group of students also attending the course.

The exercises were found in the file *exam2.html* in the *Eksamensprojekter* in <https://www.moodle.aau.dk/mod/folder/view.php?id=1812259>

I have included the exercise description for each exercise for reference. In some cases the expected output is also included.

This work can also be found on github at <https://github.com/kennethelong/Rrepo/blob/main/exam02_kel.qmd>

I have not used, or commented it out, View(xx) and mostly relied on head(xx). I have often opted to show the first 10 lines, to give better overview of the result.

Generally I have made a "local" copy of a the used data frame (df) for each exercise. This is done in an effort to ensure a high degree of repeatability. Instead of e.g. using df "wpop" from a previous exercise and manipulating it, most often a local version is made e.g. ex2x_wpop \<- wpop. Thereby I do not have to rerun previous exercises to get a df that is required as input.

Sometime the output is pasted in as a picture to ensure that even if the results change, which the could do as a result of changing e.g. how the data is cleansed, the output on which the comments are made are clear for the reader.

To enhance readability most steps are broken down and operations performed one at a time. Readability enhances durability of code, because as it may be a computer that executes the commands, it is always a human that will need to understand, change, and reuse the commands.

```{r}
## To execute this file the "here" package needs to be installed
## install.packages("here")

## Libraries required to execute this file
library(ggplot2)
library(tidyverse)
library(broom)
library(dplyr)
library(modelr)
library(doBy)
library(ggcorrplot)
library(randomForest)
library(rpart.plot)
library(Metrics)
library(MASS)
library(modelr)
```

## **Data**

This part uses the `personality` dataset from the R package `doBy` with recordings of 32 variables describing personality characteristics for 240 people. (see help file for `?personality`)

We focus on the variable `easygon` as the response variable. We split the data in training and test data as follows:

```{r}
## Loading the personality data into dat
personality_dat <- doBy::personality
## Setting a seed for reproducibility. It means that all "random" numbers will start based on the seed
set.seed(101)
## Selects half the data set
personality_i_train <- sample(nrow(personality_dat), .5*nrow(personality_dat))
## Puts one half into train
personality_train <- personality_dat[personality_i_train,]
## And the other half into test
personality_test <- personality_dat[-personality_i_train,]

```

## **Exercise 1**

Use `response_plot()` from the **development version of** `doBy` to visualize the relation between the response and the other variables in the data.

```{r, fig.height=20, fig.width=14}
## Bulding a response plot using the doBy response_plot function using the data from dat (i.e. the full data set)
responsePlot <- doBy::response_plot(easygon ~ ., data = personality_dat, geoms = c(geom_jitter(alpha = .5, width = .5))) + 
 theme(aspect.ratio = 3) + 
  facet_wrap(~ variable_name, ncol = 15) 

```

No immediate pattern emerges. Looking closely at laid back (laidbck) there could be the outline of a linear patterns, meaning that the more laid back a person is the more easy going that person may be. Let's investigate that further in coming exercises :-)

## **Exercise 3**

Specify a number of prediction models – at least one of each of the following:

1.  linear with stepwise selection (`lm` / `step`)

2.  regression tree (`rpart`)

3.  random forest (`randomForest`)

**(1) Step-wise Selection**

```{r, fig.height=12, fig.width=14}

## Building a correlation heatmap

## Calculating the correlation, rounding of with one decimal
corr_matrix <- round(cor(personality_dat), 1)

# Create correlation plot
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0))
```

The heat map shows where there are high degrees of correlation, positive (towards 1) and negative (towards -1). The strongest positive correlation are between tense and anxious or worrying, shy and quiet, outgoing and talkative or sociable. There is also a strong negative correlation between being organized and being disorganized. This last point seems intuitive.

```{r}
## The lm_full uses the data from the "train" data set and with easy going (easygon) as the response variable and all others as predictors
lm_full <- lm(easygon ~ ., data = personality_train)
## building a linear regression with no predictors
lm_1 <- lm(easygon ~ 1, data = personality_train)
## calculates the Akaike Information Criterion (AIC) for lm_full
lm_full |> AIC()
## calculates the Akaike Information Criterion (AIC) for lm_1
lm_1 |> AIC()
```

lm_full \|\> AIC() = 412.0621

lm_1 \|\> AIC() = 440.9609

This indicates that the full model (with the lower AIC of 412) is a better fit than the intercept model

```{r}
## Stepwise selection starting from lm_1 and moving forwards using the full model to determine the variables to be included
lm_forw <- step(lm_1, direction="forward", scope=terms(lm_full), trace=0)
## Backwards stepwise selection from lm_full
lm_back <- step(lm_full, direction="backward", trace=0)
## Stepwise in both directions
lm_both <- step(lm_1, direction="both", scope=terms(lm_full), trace=0)

## Creating a list of the models
model_list <- list(lm_1=lm_1, lm_forw=lm_forw, lm_both=lm_both, lm_back=lm_back, lm_full=lm_full)

## Apply the AIC for each model
model_list |> sapply(AIC)

## NOTE TO PROFESSORS -> FOR SOME UNKNOWN REASON RMSE STOPPED WORKING, SO I DEFINE THIS TEMP RMSE
temp_rmse <- function(model, data) {
    predictions <- predict(model, newdata = data)
    sqrt(mean((data$easygon - predictions)^2, na.rm = TRUE))
}

## Apply the root mean squared error (rmse) on the train data set
model_list |> sapply(function(x) temp_rmse(x, personality_train))

## Apply the rmse on the test set
model_list |> sapply(function(x) temp_rmse(x, personality_test))
```

Output inserted to be able to reference, even if the base data were to change

![](images/clipboard-3548421976.png){width="327"}

The first row are the output of the AIC on the model_list. A low AIC score indicates a better fit than a higher AIC. AIC on the full model was 412.0621 using the lm function and the score on lm_1 mode was 440.

With the 3 additional models, forward, backward and both directional selection, the forward (lm_forw) and both ways (lm_bith) model both produce a AIC score of 382.9833, indicating a better fit.

The root mean squared error (rmse) calculated on the training data (the data the models were trained on) indicates a good fit on the full model (rmse = 1.0233), followed by the backward model (1.0592).

The rmse on the test data, i.e. new data, indicates a good fit with both the forward and both directional models (rmse = 1.3497) followd by the backward model (rmse = 1.4436). The null model (lm_1) performs worst (rmse = 1.6958)

**(2) Regression Tree**

1.  **Full Tree**: Firstly the full tree is build (based in training data)
2.  **Pruning**: After reviewing the full tree and accompanying complexity parameters (CP), we'll find a suitable cp for the pruning
3.  **Prediction**: To see how powerfully model can predict it will run on the test data set
4.  **Model Evaluation**: A calculation will be made to identify the model's accuracy

```{r}
## Fit a regression tree with the training data
personality_full_tree <- rpart(easygon ~ ., data = personality_train, cp = 0, minsplit = 2, minbucket = 1)

personality_full_tree
```

```{r}
## Visualizing the regression tree
rpart.plot(personality_full_tree, digits = 3)

## Printing the table of complexity parameters (CP)
printcp(personality_full_tree)
```

This tree doesn't tell us much, it's too complex. The print of the CP Plot on the other hand provides us with some useful insights - see next section.

```{r}
## Printing the table of complexity parameters (CP)
plotcp(personality_full_tree)
```

![](images/clipboard-2333061659.png){width="480"}

Looking at the complexity parameters gives us an understanding of how many splits provides a low xerror. The lowest xerror (0.88545) is at split 2 which has a CP of 0.0561436. This is a good candidate as it 1) works with few splits and 2) has a low xerror

```{r}
## Making a local copy of the test data
personality_test_rt <- personality_test
## Converting easygon
personality_test_rt$easygon <- as.numeric(as.character(personality_test_rt$easygon))

# Predicting based on the test data
personality_full_tree_predictions <- predict(personality_full_tree, newdata = personality_test_rt)

## print(personality_full_tree_predictions)

## Calculating the RMSE for the full tree
model_rmse_full_tree <- rmse(personality_test$easygon, personality_full_tree_predictions)
print(model_rmse_full_tree)

```

The RMSE is 1.7 (1.693123.)

Let's see if we can improve that with pruning :-)

**Pruning the Tree**

Using the baseline tree, now a pruning is performed to identify a better complexity parameter.

```{r}
## The cp value comes from the output of the printcp where the lowest xerror os 0.88545 and the cp value is 0.0561436
cp_value <- 0.056

## Using the cp value with the prune funtion on the original tree
personality_rpart_prune <- prune(personality_full_tree, cp = cp_value)

par(mfrow = c(2,1))

## Shwoing the full tree
rpart.plot(personality_full_tree, main = "Full tree")
par(mfrow = c(1,1))
## Showing the pruned tree based on the cp value
rpart.plot(personality_rpart_prune, main = paste("Pruned tree, cp:", cp_value))
```

From the pruned tree it possible to conclude that the primary predictor for easy going (easygon) people is the laid back (laidbck) score.

This is in line with earlier conclusions (e.g. response plot).

People who score less than 6 of being laid back, have an average score 5.4 for easy going. Within this group relaxed (relaxed) becomes important. People scoring below 6 on relaxed, score 5.2 on easy going.

People with a laid back score above 6 score 6.8 on easy going. Among this group tenseness (tense) becomes important. People scoring below 7 on tenseness score 4 on easy going, while people scoring 7 or above on tenseness score 6.9 on easy going.

**Prediction model**

```{r}
# Predict the class for test data
personality_prune_tree_predictions <- predict(personality_rpart_prune, newdata = personality_test_rt)

## print(personality_full_tree_predictions)

## Calculating the RMSE for the full tree
model_rmse_prune_tree <- rmse(personality_test$easygon, personality_prune_tree_predictions)
print(model_rmse_prune_tree)
```

The RMSE of 1.51 means that the model will predict a e.g. 5 being some between 3.5-6.5 on average.

Not overly convincing but better than the RMSE 1.7 from the unpruned tree.

**Calculating the Accuracy of the Prediction**

```{r}
# Create a confusion matrix 
confusion_matrix_tree <- table(predicted = personality_prune_tree_predictions, actual = personality_test$easygon) 
print(confusion_matrix) 
```

![](images/clipboard-2936182842.png){width="206"}

The model predicts several values being in neighboring categories, suggesting that the models does not offer very correct prediction.

```{r}
# Calculate accuracy 
accuracy <- sum(diag(confusion_matrix_tree)) / sum(confusion_matrix_tree) 
print(paste("Accuracy: ", round(accuracy, 3)))
```

The accuracy of the model is calculated to be 10.8%

Overall the model does seem to offer a powerful prediction mechanism.

### **(3) Random Forest**

For the random forest model, firstly a baseline model is developed using randomForest trained on the training data to create a base line.

### **Baseline RF model**

```{r}
## Setting the seed
set.seed(123)

## Making a local copy of train and test data
personality_train_ex3 <- personality_train
personality_test_ex3 <- personality_test

## Setting easygoing as a factor
personality_train_ex3$easygon <- as.factor(personality_train_ex3$easygon)
personality_test_ex3$easygon <- as.factor(personality_test_ex3$easygon)

## Doing a random forest on the training data set, with 500 trees, and setting type to classification (and not regresseion)
baseline_rf <- randomForest(easygon ~ ., data = personality_train_ex3, 
                         importance = TRUE, 
                         ntree = 500, 
                         type = `classification`) 

print(baseline_rf)
```

```{r, fig.height=7}
## Building and showing a variable importance plot
varImpPlot(baseline_rf)
```

MeanDecreaseAccurency (left graph) shows the personality traits on the y-axis and the mean decrease in accurency on the x-axis. Laid back (laidbck) at the top indicates that this variable is influential when predicting if a person is easy going. The same is true for approving (approvn).

MeanDecreaseGini (right graph) shows personality traits on the y-axis and mean decrease in Gini on the y-axis. Laid back (laidbck) again scores at the top indicating influence on predicting if a person is easy going. The next variable in this graph is relaxed (relaxed).

In both graphs top 5 are laid back at the top. The next 4 variables (approv, persev, relaxed, anxious) are in both graphs but in different places.

This indicates that the most powerful predictor is laid back and then a combination of approv, persev, relaxed and anxious.

### **Predicting and Calculating RMSE for the Baseline RF model**

```{r}
## Predicting based on test data
##personality_rf_predictions <- predict(personality_rf_model, newdata = personality_test_ex3)
baseline_predictions <- predict(baseline_rf, newdata = select(personality_test_ex3, -easygon))

baseline_predictions_numeric <- as.numeric(as.character(baseline_predictions))
actual_numeric <- as.numeric(as.character(personality_test_ex3$easygon))

## Calculating RMSE
model_rmse_rf_baseline <- rmse(actual_numeric, baseline_predictions_numeric)

print(model_rmse_rf_baseline)
```

The RMSE is 1.53 for the baseline model.

Let's see if we can improve that tuning the model.

### **Tuning the Model**

```{r}
## Restating the seed
set.seed(123)

## Using the tune random forest forest function on the training set. This helps find the number of predictors that gives the an overview of OOB for a set of predictors
personality_rf_tune <- tuneRF(y = personality_train$easygon, x = select(personality_test_ex3, -easygon), improve = 0.001)

## print(personality_rf_tune)

best_mtry <- personality_rf_tune[which.min(personality_rf_tune[, 2]), 1]

best_mtry
```

Based on the above, using 5 predictors (mtry = 5) provides the model with the lowest out of the bag error (OOB) of 2.37 (2.373505), making it the best performing model.

```{r}
## Redoing the random forst with mtry = optimal best option (5)
final_rf_mtry <- randomForest(
  easygon ~ ., 
  data = personality_train, 
  mtry = best_mtry, 
  ntree = 500
)

## Predicting based on tuend model and the test data
tuned_predictions <- predict(final_rf_mtry, newdata = select(personality_test_ex3, -easygon))

## Calculating RMSE
model_rmse_rf_tuned <- rmse(actual_numeric, tuned_predictions)

model_rmse_rf_tuned

```

Before tuning the RMSE was 1.53 and the tuned model is now at 1.42, so an overall improvement.

```{r}
## Restating the seed
set.seed(123)

## Tuning the random forest to find the optimal values of hyperparamters for the RF model.
personality_tune_rf_one <- function(n){
  output <- tuneRF(y = personality_train$easygon, x = select(personality_test_ex3, -easygon), 
                   ntreeTry = n, improve = 0.05, trace = FALSE, plot = FALSE)
  return(bind_cols(nTree = n, output))
}

## Setting a number of trees
nTree <- c(10, 50, 100, 200, 500, 1000)

## Using the map function to applyt the personality_tune_one function to each number of trees in the nTree list
OOB_error <- map(nTree, personality_tune_rf_one)
```

```{r}
## Building a plot with the output of the previous OOB_error list against each number of trees and mtrys (number of variables to consider at each split)
OOB_error |> 
  bind_rows() |> 
  ggplot(aes(x = mtry, y = OOBError, colour = factor(nTree))) + 
  geom_point() + geom_line()
```

**Calculating the Accuracy of the Prediction**

```{r}
# Create a confusion matrix
confusion_matrix <- table(predicted = personality_rf_predictions, actual = personality_test$easygon)
print(confusion_matrix)

```

This outlines predicted values versus actual values. For predicted value 6, 11 predictions were correct. It however incorrectly guess a large number especially of 7 (13) and 8 (8). These is cases where the model guess them to be 6 but they were in fact either 7 or 8.

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy: ", round(accuracy, 3)))
```

Overall the model can predict in 26.7% of the cases. This is low and the model thus does not overall have a high level of performance.

## **Exercise 4**

For **one of** the linear models show the estimated coefficients and comment **briefly** on the output (for example, comment on the sign of the estimates).

```{r}
summary(lm_forw)
```

This is a linear model that moves forward to identify the best possible model.

The intercept of this model is at -0.01123.

The laid back (laidbck) variable has a positive coefficient of 0.37 (0.36568), meaning that if easy going trait (easygon) increases one unit, laid back trait increases by 0.37. The standard error (std.error) for laid back is 0.08 (0.07899) indicating a high degree of reliability.

Cooperativeness (coopera) also have a significant positive coefficient of 0.38 (0.38126) and a fairly low standard error of 0.1 (0.09883), indicating that it's less reliable than laid back.

The two negative predictors persevering (persevr) and harsh (harsh) are negatively associated with easygoingness in this model.

## **Exercise 5**

Compare the performance of the fitted models by predicting the values of `easygon` for the test data and show a plot comparing the predictions to the observations from the test data. Also make a table of the RMSE for each model.

**The Plot does not work as it should**

```{r}
### - This does not work as it should


## Making the full model list
model_list_ex5 <- list(
  lm_1=lm_1, 
  lm_forw=lm_forw, 
  lm_both=lm_both, 
  lm_back=lm_back, 
  lm_full=lm_full

)

## Observed easygoingness in test data
results <- data.frame(obs = personality_test$easygoing)

# Function to predict values for each model
get_predictions <- function(model, data) {
  return(predict(model, newdata = data))
}

model_name <- names(model_list_ex5)

# Add predicted values for each model to the results data frame
for (model_name in names(model_list_ex5)) {
  results[[model_name]] <- get_predictions(model_list_ex5[[model_name]], personality_test)
}

ggplot(model_list_ex5, aes(x = easygon, y = pred, color = method))+
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  facet_wrap(~method, ncol = 3) 
```

**Table of RMSE for each model**

```{r}
## Setting all required RMSEs
model_rmse_lm <- sapply(model_list, function(model) temp_rmse(model, personality_test))
model_rmse_full_tree <- model_rmse_full_tree
model_rmse_prune_tree <- model_rmse_prune_tree
model_rmse_rf_baseline <- model_rmse_rf_baseline 
model_rmse_rf_tuned <- model_rmse_rf_tuned

# Combine all RMSE values into a single named vector
model_rmse <- c(model_rmse_lm,
                 full_tree = model_rmse_full_tree,
                 prune_tree = model_rmse_prune_tree,
                 rf_baseline = model_rmse_rf_baseline,
                 rf_tuned = model_rmse_rf_tuned)

# Create a data frame from the RMSE values
rmse_table <- data.frame(
  Model = names(model_rmse),
  RMSE = model_rmse
)

sorted_rmse_table <- rmse_table |>
  arrange(RMSE) |> 
   mutate(RMSE = round(RMSE, 3))

# Print the table
print(sorted_rmse_table)
```

# **Part II - 2: Classification**

The dataset `diabetes.csv` is about risk factors for diabetes and contains 768 observations of 9 variables. The response variable is `Outcome` which is binary. The goal is to predict `Outcome` using the other variables. A description of the dataset can be found at <https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data>

## **Exercise 1**

Read in and clean the data by detecting unexpected values of some variables, replace the values with `NA` and finally make a data set with only the complete cases.

```{r}
## Reading the csv file diabetes.csv.csv into diabetesDataset_csvv. The file is in the root folder, where this file is also found.
diabetesDataset_csv <- read_csv("diabetes.csv", col_names = TRUE, col_types = NULL)

diabetesDataset <- diabetesDataset_csv ## Making a separate dataset to be cleaned

## Cleaning the dataset setting incorret values to NA, if insulin is 0, skin thinckness is 0, ages is below 21, if BMI is below 15 and if blood pressure is below 20. Lastely removing all NA values
diabetesDataset$Insulin[diabetesDataset$Insulin < 1] <- NA
diabetesDataset$SkinThickness[diabetesDataset$SkinThickness < 1] <- NA
diabetesDataset$Age[diabetesDataset$Age < 21] <- NA
diabetesDataset$BMI[diabetesDataset$BMI < 15] <- NA
diabetesDataset$BloodPressure[diabetesDataset$BloodPressure < 20] <- NA
diabetesDataset_clean <- na.omit(diabetesDataset)

## dim(diabetesDataset_clean)

head(diabetesDataset_clean, 10)

```

The result is 393 and 9. Is is different from the 392 and 9 in the exam set, off by one data point.

## **Exercise 2**

Plot the response against each of the explanatory variables/features and add a smooth line as a very rough indication of how the probability of diabetes depends on the individual feature (note the response has to be numeric for the smooth line to work).

```{r}
## Bulding a response plot using the doBy response_plot function using the data from dat (i.e. the full data set)
diabetesResponse = doBy::response_plot(Outcome ~ ., data = diabetesDataset_clean, geoms = c(geom_jitter(alpha = .1, width = .1), geom_smooth()))+ 
  facet_wrap(~ variable_name)

```

## **Exercise 3**

Ensure `Outcome` is a `factor` and split the data in a training and test set as follows:

```         
set.seed(202) # for reproducibility 
i_train <- sample(nrow(dat), .5*nrow(dat)) 
train <- dat[i_train,] 
test <- dat[-i_train,]
```

Specify a number of classification models and report the corresponding confusion matrix for the test data. You must include at least one of each of the following:

1.  logistic regression with stepwise selection (`glm` / `step`)

2.  regression tree (`rpart`)

3.  random forest (`randomForest`)

4.  linear/quadratic discriminant analysis

Time ran out, so not all analysis are performed.

```{r}
set.seed(202) # for reproducibility  
diabetes_i_train <- sample(nrow(diabetesDataset_clean), .5*nrow(diabetesDataset_clean))  
diabetes_train <- diabetesDataset_clean[diabetes_i_train,]  
diabetes_test <- diabetesDataset_clean[-diabetes_i_train,]

## Creating a plot where 1 is set to "has diabetes" and 0 set to "does not have diabetes", using glucose as predictor
diabetesDataset_clean |>
  ggplot(aes(x = Glucose, y = factor(Outcome, levels = c(0, 1), labels = c("Does not have diabetes", "Has diabetes")))) +
  geom_jitter(width = .2, height = .2, alpha = .2) +
  labs(y = "Outcome", x = "Glucose")
```

This does not give the impression that glucose in and of itself can predict diabetes, though it may have a significant effect.

```{r}
diabetesTable <- diabetesDataset_clean |> 
  group_by(Glucose, Outcome) |> 
  summarise(n = n()) |> 
  mutate(prop = n / sum(n))

props <- diabetesTable |> 
  filter(Outcome == 1)

prop_plot <- diabetesTable |> ggplot(aes(x = Glucose, y = Outcome)) +
  geom_point(alpha = 0) + 
  geom_point(aes(y = 1+prop), data = props, col = "red")
prop_plot

```

```{r}     ## Fitting a LDA model based on the training  diabetes_lda_baseline <- lda(Outcome ~ ., data=diabetes_train)}
```

### Logistic Regression with Step Wise Selection

**Setting up the Model**

```{r}
## Fitting a logistic regression model based on the training  
diabetes_logreg_baseline <- glm(Outcome ~ ., data=diabetes_train, family = binomial)

info <- tidy(diabetes_logreg_baseline) |> 
  mutate(low_ci = estimate-1.96*std.error, high_ci = estimate+1.96*std.error) |> 
  mutate(exp_low_ci = exp(low_ci), exp_high_ci = exp(high_ci))

info

summary(diabetes_logreg_baseline)
```

Glucose stands out becuse of a combination of things 1) it has a low standard error of 0.0085 and 2) the estimate is 4.1, meaning that for every unit increase in glucose risk of having diabetes increases by 0.041

**Predicting**

```{r}
ex3_diabetes_train <- diabetes_train
ex3_diabetes_test <- diabetes_test
ex3_diabetes_train$Outcome <- as.factor(diabetes_train$Outcome)
ex3_diabetes_test$Outcome <- as.factor(diabetes_test$Outcome)

## Fitting a logistic regression model based on the training   
diabetes_logreg_baseline <- predict(diabetes_logreg_baseline, newdata=ex3_diabetes_test, type = "response")

predicted_classes <- ifelse(diabetes_logreg_baseline > 0.5, 1, 0)

# Check first few predictions
head(predicted_classes)

confusion_matrix <- table(Predicted = predicted_classes, Actual = diabetes_test$Outcome)
print(confusion_matrix)

# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))
```

The logistic regression model has an accuracy of 76.14%

### Linear discriminant analysis

**Setting up the Model**

```{r}
## Fitting a LDA model based on the training 
diabetes_lda_baseline <- lda(Outcome ~ ., data=diabetes_train)
```

**Internal prediction**

```{r}
## Predicting based on test data 
diabetes_lda_baseline_int_prediction <- predict(diabetes_lda_baseline, newdata=diabetes_train)$class  

head(diabetes_lda_baseline_int_prediction, 4) 

## Doing a confusion table 
personality_lda_baseline_tab <- table(Actual = diabetes_train$Outcome, Predicted = diabetes_lda_baseline_int_prediction) 

personality_lda_baseline_tab   

sum(diag(personality_lda_baseline_tab))/sum(personality_lda_baseline_tab)
```

The accuracy for the internal prediction is 80.6%

**External prediction**

```{r}
## Predicting based on test data
diabetes_lda_baseline_ext_prediction <- predict(diabetes_lda_baseline, newdata=diabetes_test)$class

head(diabetes_lda_baseline_ext_prediction, 4)

## Doing a confusion table
personality_lda_baseline_tab <- table(Actual = diabetes_test$Outcome, Predicted = diabetes_lda_baseline_ext_prediction)

personality_lda_baseline_tab

sum(diag(personality_lda_baseline_tab))/sum(personality_lda_baseline_tab)

```

The accuracy for the external prediction is 76.6%

The external prediction based on the test data is lower than the internal which is based on the training data. Since the model is trained on the same data as is used to predict in the internal model, it would be expected that the accuracy would be higher.
