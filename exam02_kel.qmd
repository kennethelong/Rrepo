---
title: "Exam01"
author: "Kenneth"
format: html
editor: visual
embed-ressources: true
---

**Data science exam: part 2**

# **Part II - 1: Regression**

**Introduction - Read Me**

This file contains the work of **Kenneth Elong**. It was submitted for review on the 15th of October.

All exercises were solved by Kenneth Elong. No one else have provided any code for the solving of the exercises. I did however discuss possible solutions and errors I encountered with a group of students also attending the course.

The exercises were found in the file *exam2.html* in the *Eksamensprojekter* in <https://www.moodle.aau.dk/mod/folder/view.php?id=1812259>

I have included the exercise description for each exercise for reference. In some cases the expected output is also included.

This work can also be found on github at <https://github.com/kennethelong/Rrepo/blob/main/exam02_kel.qmd>

I have not used, or commented it out, View(xx) and mostly relied on head(xx). I have often opted to show the first 10 lines, to give better overview of the result.

Generally I have made a "local" copy of a the used df for each exercise. This is done in an effort to ensure a high degree of repeatability. Instead of e.g. using df "wpop" from a previous exercise and manipulating it, most often a local version is made e.g. ex2x_wpop \<- wpop. Thereby I do not have to rerun previous exercises to get a df that is required as input.

To enhance readability steps are broken down and operations performed one at a time. Readability enhances durability of code, because as it may be a computer that executes the commands, it is always a human that will need to understand, change, and reuse the commands.

```{r}
## To execute this file the "here" package needs to be installed
## install.packages("here")

## Libraries required to execute this file
library(ggplot2)
library(tidyverse)
library(broom)
library(dplyr)
library(modelr)
library(doBy)
library(ggcorrplot)
library(randomForest)
library(rpart.plot)
library(Metrics)
```

## **Data**

This part uses the `personality` dataset from the R package `doBy` with recordings of 32 variables describing personality characteristics for 240 people. (see help file for `?personality`)

We focus on the variable `easygon` as the response variable. We split the data in training and test data as follows:

```{r}
## Loading the personality data into dat
personality_dat <- doBy::personality
## Setting a seed for reproducibility. I means that all "random" numbers will start based on the seed
set.seed(101)
## Selects half the data set
personality_i_train <- sample(nrow(personality_dat), .5*nrow(personality_dat))
## Puts one half into train
personality_train <- personality_dat[personality_i_train,]
## And the other  half into test
personality_test <- personality_dat[-personality_i_train,]

```

## **Exercise 1 - DONE**

Use `response_plot()` from the **development version of** `doBy` to visualize the relation between the response and the other variables in the data.

```{r, fig.height=20, fig.width=14}
## Bulding a response plot using the doBy response_plot function using the data from dat (i.e. the full data set)
responsePlot <- doBy::response_plot(easygon ~ ., data = personality_dat, geoms = c(geom_jitter(alpha = .5, width = .5))) + 
 theme(aspect.ratio = 3) + 
  facet_wrap(~ variable_name, ncol = 15) 

```

No immediate pattern emerges. Looking closely at laid back (laidbck) there could be the outline of a linear patterns, meaning that the more laid back a person is the more easy going that person may be. Let's investigate that further in coming exercises :-)

## **Exercise 3**

Specify a number of prediction models – at least one of each of the following:

1.  linear with stepwise selection (`lm` / `step`) - ALMOST DONE

2.  regression tree (`rpart`) - MISSING PREDICTION

3.  random forest (`randomForest`) - DONE

**(1) Step-wise Selection**

```{r, fig.height=12, fig.width=14}

## Building a correlation heatmap

## Calculating the correlation, rounding of with one decimal
corr_matrix <- round(cor(personality_dat), 1)

# Create correlation plot
ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0))
```

The heat map shows where there are high degrees of correlation, positive (towards 1) and negative (towards -1). The strongest positive correlation are between tense and anxious or worrying, shy and quiet, outgoing and talkative or sociable. There is also a strong negative correlation between being organized and being disorganized. This last point seems intuitive.

```{r}
## The lm_full uses the data from the "train" data set and with easy going (easygon) as the response variable and all others as predictors
lm_full <- lm(easygon ~ ., data = personality_train)
## building a linear regression with no predictors
lm_1 <- lm(easygon ~ 1, data = personality_train)
## calculates the Akaike Information Criterion (AIC) for lm_full
lm_full |> AIC()
## calculates the Akaike Information Criterion (AIC) for lm_1
lm_1 |> AIC()


```

lm_full \|\> AIC() = 412.0621

lm_1 \|\> AIC() = 440.9609

This indicates that the full model (with the lower AIC of 412) is a better fit than the intercept model

```{r}
## Stepwise selection starting from lm_1 and moving forwards using the full model to determine the variables to be included
lm_forw <- step(lm_1, direction="forward", scope=terms(lm_full), trace=0)
## Backwards stepwise selection from lm_full
lm_back <- step(lm_full, direction="backward", trace=0)
## Stepwise in both directions
lm_both <- step(lm_1, direction="both", scope=terms(lm_full), trace=0)

## Creating a list of the models
model_list <- list(lm_1=lm_1, lm_forw=lm_forw, lm_both=lm_both, lm_back=lm_back, lm_full=lm_full)

## Apply the AIC for each model
model_list |> sapply(AIC)

## NOTE TO PROFESSORS -> FOR SOME UNKNOWN REASON RMSE STOPPED WORKING, SO I DEFINE THIS TEMP RMSE
temp_rmse <- function(model, data) {
    predictions <- predict(model, newdata = data)
    sqrt(mean((data$easygon - predictions)^2, na.rm = TRUE))
}

## Apply the root mean squared error (rmse) on the train data set
model_list |> sapply(function(x) temp_rmse(x, personality_train))

## Apply the rmse on the test set
model_list |> sapply(function(x) temp_rmse(x, personality_test))
```

Output inserted to be able to reference, even if the base data were to change

![](images/clipboard-3548421976.png){width="327"}

The first row are the output of the AIC on the model_list. A low AIC score indicates a better fit than a higher AIC. AIC on the full model was 412.0621 using the lm function and the score on lm_1 mode was 440.

With the 3 additional models, forward, backward and both directional selection, the forward (lm_forw) and both ways (lm_bith) model both produce a AIC score of 382.9833, indicating a better fit.

The root mean squared error (rmse) calculated on the training data (the data the models were trained on) indicates a good fit on the full model (rmse = 1.0233), followed by the backward model (1.0592).

The rmse on the test data, i.e. new data, indicates a good fit with both the forward and both directional models (rmse = 1.3497) followd by the backward model (rmse = 1.4436). The null model (lm_1) performs worst (rmse = 1.6958)

**(2) Regression Tree**

1.  **Full Tree**: Firstly the full tree is build
2.  **Pruning**: After reviewing the full tree and accompanying complexity parameters (CP), we'll find a suitable cp for the pruning
3.  **Prediction**: To see how powerfully model can predict it will run on the test data set
4.  **Model Evaluation**: A calculation will be made to identify the model's accurency

```{r}
## Fit a regression tree with the training data
personality_rpart <- rpart(easygon ~ ., data = personality_train, cp = 0, minsplit = 2, minbucket = 1)

personality_rpart
```

```{r}
## Visualizing the regression tree
rpart.plot(personality_rpart, digits = 3)

## Printing the table of complexity parameters (CP)
printcp(personality_rpart)

```

This tree doesn't tell us much, it's too complex. The print of the CP Plot on the other hand provides us with some useful insights - see next section.

```{r}
## Printing the table of complexity parameters (CP)
plotcp(personality_rpart)
```

![](images/clipboard-961595202.png){width="421"}

Looking at the complexity parameters gives us an understanding of how many splits provides a low xerror. The lowest xerror (0.94679) is at split 2 which has a CP of 0.0561436. This is a good candidate as it 1) works with few splits and 2) has a low xerror

```{r}
## The cp value comes from the output of the printcp where the lowest xerror os 0.94679 and the cp value is 0.0561436
cp_value <- 0.056

## Using the cp value with the prune funtion on the original tree
personality_rpart_prune <- prune(personality_rpart, cp = cp_value)

par(mfrow = c(2,1))

## Shwoing the full tree
rpart.plot(personality_rpart, main = "Full tree")
par(mfrow = c(1,1))
## Showing the pruned tree based on the cp value
rpart.plot(personality_rpart_prune, main = paste("Pruned tree, cp:", cp_value))


```

From the pruned tree it possible to conclude that the primary predictor for easy going (easygon) people is the laid back (laidbck) score.

This is in line with earlier conclusions.

People who score less than 6 of being laid back, have an average score 5.4 for easy going. Within this group relaxed (relaxed) becomes important. People scoring below 6 on relaxed, score 5.2 on easy going.

People with a laid back score above 6 score 6.8 on easy going. Among this group tenseness (tense) becomes important. People scoring below 7 on tenseness score 4 on easy going, while people scoring 7 or above on tenseness score 6.9 on easy going.

**Prediction model**

```{r}
# Predict the class for test data
personality_tree_predictions <- predict(personality_rpart_prune, newdata = personality_test)

print(personality_rf_predictions)

model_rmse_tree <- rmse(personality_test$easygon, personality_tree_predictions)
print(model_rmse_tree)
```

The RMSE of 1.51 means that the model will predict a e.g. 5 being some between 3.5-6.5 on average.

**Calculating the Accuracy of the Prediction**

```{r}
# Create a confusion matrix 
confusion_matrix_tree <- table(predicted = personality_tree_predictions, actual = personality_test$easygon) 
print(confusion_matrix) 
```

Rounded (5.15 = 5.0, 5.54 = 5.5, 6.9464 = 7.0) for sake of making interpret.

The model predicts several values being in neighboring categories, suggesting that the models does not offer very correct prediction.

```{r}
# Calculate accuracy 
accuracy <- sum(diag(confusion_matrix_tree)) / sum(confusion_matrix_tree) 
print(paste("Accuracy: ", round(accuracy, 3)))
```

The accuracy of the model is calculated to be 10.8%

Overall the model does seem to offer

### **(3) Random Forest**

```{r}
## Setting the seed
set.seed(123)

## Making a local copy
personality_train_ex3 <- personality_train
personality_test_ex3 <- personality_test

## Setting easygoing as a factor
personality_train_ex3$easygon <- as.factor(personality_train_ex3$easygon)
personality_test_ex3$easygon <- as.factor(personality_test_ex3$easygon)

## Doing a random forest on the training data set, with 500 trees, and setting type to classification (and not regresseion)
personality_rf_model <- randomForest(easygon ~ ., data = personality_train_ex3, 
                         importance = TRUE, 
                         ntree = 500, 
                         type = `classification`) 

print(personality_rf_model)

```

```{r, fig.height=7}
## Building and showing a variable importance plot
varImpPlot(personality_rf_model)
```

MeanDecreaseAccurency (left graph) shows the personality traits on the y-axis and the mean decrease in accurency on the x-axis. Laid back (laidbck) at the top indicates that this variable is influential when predicting if a person is easy going. The same is true for approving (approvn).

MeanDecreaseGini (right graph) shows personality traits on the y-axis and mean decrease in Gini on the y-axis. Laid back (laidbck) again scores at the top indicating influence on predicting if a person is easy going. The next variable in this graph is relaxed (relaxed).

In both graphs top 5 are laid back at the top. The next 4 variables (approv, persev, relaxed, anxious) are in both graphs but in different places.

This indicates that the most powerful predictor is laid back and then a combination of approv, persev, relaxed and anxious.

```{r}
## Restating the seed
set.seed(123)

## Using the tune random forest forest function on the training set. This helps find the number of predictors that gives the an overview of OOB for a set of predictors
personality_RF_tune <- tuneRF(y = personality_train$easygon, x = select(personality_train, -easygon), improve = 0.001)

print(personality_RF_tune)
```

Based on the above, using 5 predictors (mtry = 5) provides the model with the lowest out of the bag error (OOB) of 1.78 (1.782553), making it the best performing model.

```{r}
## Restating the seed
set.seed(123)

## Tuning the random forest to find the optimal values of hyperparamters for the RF model.
personality_tune_one <- function(n){
  output <- tuneRF(y = personality_train$easygon, x = select(personality_train, -easygon), 
                   ntreeTry = n, improve = 0.05, trace = FALSE, plot = FALSE)
  return(bind_cols(nTree = n, output))
}

## Setting a number of trees
nTree <- c(10, 50, 100, 200, 500, 1000)

## Using the map function to applyt the personality_tune_one function to each number of trees in the nTree list
OOB_error <- map(nTree, personality_tune_one)
```

```{r}
## Building a plot with the output of the previous OOB_error list against each number of trees and mtrys (number of variables to consider at each split)
OOB_error |> 
  bind_rows() |> 
  ggplot(aes(x = mtry, y = OOBError, colour = factor(nTree))) + 
  geom_point() + geom_line()
```

**Prediction Based on Test Data**

```{r}
# Predict the class for test data
personality_rf_predictions <- predict(personality_rf_model, newdata = personality_test_ex3, type = "class")

print(personality_rf_predictions)
```

**Calculating the Accuracy of the Prediction**

```{r}
# Create a confusion matrix
confusion_matrix <- table(predicted = personality_rf_predictions, actual = personality_test$easygon)
print(confusion_matrix)

```

This outlines predicted values versus actual values. For predicted value 6, 11 predictions were correct. It however incorrectly guess a large number especially of 7 (13) and 8 (8). These is cases where the model guess them to be 6 but they were in fact either 7 or 8.

```{r}
# Calculate accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(paste("Accuracy: ", round(accuracy, 3)))
```

Overall the model can predict in 26.7% of the cases. This is low and the model thus does not overall have a high level of performance.

## **Exercise 4 - DONE**

For **one of** the linear models show the estimated coefficients and comment **briefly** on the output (for example, comment on the sign of the estimates).

```{r}
summary(lm_forw)
```

This is a linear model that moves forward to identify the best possible model.

The intercept of this model is at -0.01123.

The laid back (laidbck) variable has a positive coefficient of 0.37 (0.36568), meaning that if easy going trait (easygon) increases one unit, laid back trait increases by 0.37. The standard error (std.error) for laid back is 0.08 (0.07899) indicating a high degree of reliability.

Cooperativeness (coopera) also have a significant positive coefficient of 0.38 (0.38126) and a fairly low standard error of 0.1 (0.09883), indicating that it's less reliable than laid back.

The two negative predictors persevering (persevr) and harsh (harsh) are negatively associated with easygoingness in this model.

## **Exercise 5**

Compare the performance of the fitted models by predicting the values of `easygon` for the test data and show a plot comparing the predictions to the observations from the test data. Also make a table of the RMSE for each model.

full_model_list \<- data.frame(model_list_clean)

```{r}
### - This does not work as it should


## Making the full model list
model_list_ex5 <- list(
  lm_1=lm_1, 
  lm_forw=lm_forw, 
  lm_both=lm_both, 
  lm_back=lm_back, 
  lm_full=lm_full

)

## Observed easygoingness in test data
results <- data.frame(obs = personality_test$easygoing)

# Function to predict values for each model
get_predictions <- function(model, data) {
  return(predict(model, newdata = data))
}

model_name <- names(model_list_ex5)

# Add predicted values for each model to the results data frame
for (model_name in names(model_list_ex5)) {
  results[[model_name]] <- get_predictions(model_list_ex5[[model_name]], personality_test)
}

ggplot(model_list_ex5, aes(x = easygon, y = pred, color = method))+
  geom_point() +
  geom_abline(slope = 1, intercept = 0, color = "black") +
  facet_wrap(~method, ncol = 3) 
```

**Table of RMSE for each model**

```{r}
# Predict the class for test data
personality_lm_pred <- predict(lm_full, newdata = personality_test)

print(personality_lm_pred)
```

# **Part II - 2: Classification**

The dataset `diabetes.csv` is about risk factors for diabetes and contains 768 observations of 9 variables. The response variable is `Outcome` which is binary. The goal is to predict `Outcome` using the other variables. A description of the dataset can be found at <https://www.kaggle.com/datasets/mathchi/diabetes-data-set/data>

## **Exercise 1**

Read in and clean the data by detecting unexpected values of some variables, replace the values with `NA` and finally make a data set with only the complete cases.

```{r}
## Reading the csv file diabetes.csv.csv into diabetesDataset_csvv. The file is in the root folder, where this file is also found.
diabetesDataset_csv <- read_csv("diabetes.csv", col_names = TRUE, col_types = NULL)

diabetesDataset <- diabetesDataset_csv ## Making a separate dataset to be cleaned

## Cleaning the dataset setting incorret values to NA, if insulin is 0, skin thinckness is 0, ages is below 21, if BMI is below 15 and if blood pressure is below 20. Lastely removing all NA values
diabetesDataset$Insulin[diabetesDataset$Insulin < 1] <- NA
diabetesDataset$SkinThickness[diabetesDataset$SkinThickness < 1] <- NA
diabetesDataset$Age[diabetesDataset$Age < 21] <- NA
diabetesDataset$BMI[diabetesDataset$BMI < 15] <- NA
diabetesDataset$BloodPressure[diabetesDataset$BloodPressure < 20] <- NA
diabetesDataset_clean <- na.omit(diabetesDataset)

dim(diabetesDataset_clean)

head(diabetesDataset_clean, 10)

```

The result is 393 and 9. Is is different from the 392 and 9 in the exam set, off by one data point.

## **Exercise 2**

Plot the response against each of the explanatory variables/features and add a smooth line as a very rough indication of how the probability of diabetes depends on the individual feature (note the response has to be numeric for the smooth line to work).

```{r}
## features <- names(diabetesDataset_clean)[names(diabetesDataset_clean) != "Outcomes"]

## Bulding a response plot using the doBy response_plot function using the data from dat (i.e. the full data set)
## doBy::response_plot(Outcome ~ ., data = diabetesDataset_clean, geoms = c(geom_jitter(alpha = .1, width = .1))) + 
 ##theme(aspect.ratio = .1) + 
##   geom_point() +
##   geom_smooth()+ 
##   facet_wrap(~ variable_name) 

## Bulding a response plot using the doBy response_plot function using the data from dat (i.e. the full data set)
diabetesResponse = doBy::response_plot(Outcome ~ ., data = diabetesDataset_clean, geoms = c(geom_jitter(alpha = .1, width = .1), geom_smooth()))+ 
  facet_wrap(~ variable_name)

## diabetesResponse + geom_point() +
##   geom_smooth()+ 
##   facet_wrap(~ variable_name)

## diabetesDataset_clean |> doBy::response_plot(Outcome ~ ., geoms = c(geom_jitter(alpha = .1, width = .1))) + 
##   geom_point() +
##   geom_smooth()+ 
##   facet_wrap(~ variable_name) 


## lapply(features, function(feature) {
##   ggplot(diabetesDataset_clean, aes(x = feature, y = "Outcome")) +
##     geom_point(alpha = 0.4) +     # Scatter plot for data points
##     geom_smooth(method = "loess") +  # Add a smooth line
##     labs(title = paste("Diabetes vs", feature), x = feature, y = "Probability of Diabetes") +
##     theme_minimal()
## })
```

## **Exercise 3**

Ensure `Outcome` is a `factor` and split the data in a training and test set as follows:

```         
set.seed(202) # for reproducibility 
i_train <- sample(nrow(dat), .5*nrow(dat)) 
train <- dat[i_train,] 
test <- dat[-i_train,]
```

Specify a number of classification models and report the corresponding confusion matrix for the test data. You must include at least one of each of the following:

1.  logistic regression with stepwise selection (`glm` / `step`)

2.  regression tree (`rpart`)

3.  random forest (`randomForest`)

4.  linear/quadratic discriminant analysis

```{r}
set.seed(202) # for reproducibility 
diabetes_i_train <- sample(nrow(diabetesDataset_clean), .5*nrow(diabetesDataset_clean)) 
diabetes_train <- diabetesDataset_clean[diabetes_i_train,] 
diabetes_test <- diabetesDataset_clean[-diabetes_i_train,]
```

## 
